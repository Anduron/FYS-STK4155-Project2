%================================================================
\section{Results and Discussion}\label{sec:Results}
%================================================================

The programs containing the implementation of the methods discussed in the previous section, as well as the results presented in this section, can be found at the GitHub repository
\begin{center}
    \url{https://github.com/nicolossus/FYS-STK4155-Project2}
\end{center}

The procedures for producing the following results are contained in Jupyter Notebooks found \href{https://github.com/nicolossus/FYS-STK4155-Project2/tree/master/notebooks}{here}.

%----------------------------------------------------------------
\subsection{Learning the Ising Hamiltonian with Linear Regression}\label{sec:results linreg}
%----------------------------------------------------------------
Accompanying notebook: \href{https://github.com/nicolossus/FYS-STK4155-Project2/blob/master/notebooks/linreg_ising.ipynb}{linreg\_ising.ipynb}.


\autoref{fig:j_ols} shows the coupling matrix obtained by performing OLS regression.

\begin{figure}[H]
\begin{center}\includegraphics[scale=0.6]{latex/figures/ising_J_ols.pdf}
\end{center}
\caption{Learned coupling matrix $J_{ij}$ for the Ising model ansatz in \autoref{eq:general} for ordinary least squares (OLS) regression.}
\label{fig:j_ols}
\end{figure}

In the linear regression problem, we assumed no prior knowledge about the origin of the data set. Hence, a Ising model with pairwise interactions between every pair of variables is used here. This generalized model gives rise to that OLS learn nearly symmetric weights $J=-0.5$, as seen in \autoref{fig:j_ols}. What the generalization of the model means, is that we included both $s_j s_i$ and $s_i s_j$ for all $i,j$ in the features. Since these quantities are equal, we do not get a unique solution for OLS, so we resort to singular value decomposition (SVD). SVD gives the coefficients with the least $L2$-norm. Hence we tend to get solutions $J_{i,j}=J_{j,i}=-0.5$.

\autoref{fig:j_lmbda} shows the coupling matrices obtained by performing Ridge and Lasso regression with different regularization parameter, $\lambda$, values.

\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\centering
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_0_0001.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_0_001.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_0_01.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_0_1.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_1_0.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_10_0.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_100_0.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_1000_0.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_10000_0.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.35]{latex/figures/ising_J_lmbda_100000_0.pdf}}}
\caption{Learned coupling matrix $J_{ij}$ for the Ising model ansatz in \autoref{eq:general} for Ridge regression (to the left in each subfigure) and Lasso regression (to the right in each subfigure) at different regularization strengths $\lambda$. }
\label{fig:j_lmbda}
\end{figure}

From \autoref{fig:j_lmbda} we see that Ridge regression also learn nearly symmetric weights $J=-0.5$. Ridge is similar to the OLS case in this regard, since it uses an $L2$ penalty. Regularization of Ridge regression does not improve the performance. Lasso regression, however, tends to break this symmetry and perfectly determine the coupling $J=-1$ with proper regularization. With regularization $\lambda = 10^{-2}$, Lasso even gives an $R^2$ score of 1. 

\autoref{fig:performance_lmbda_1d} shows the $R^2$ score and MSE as a function of the regularization parameter $\lambda$.

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/ising1D_r2_vs_lmbda.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/ising1D_mse_vs_lmbda.pdf}}}
\caption{Performance of OLS, Ridge and Lasso regression with different regularization strengths on the Ising model as measured by the \textbf{(a)} $R^2$ score and \textbf{(b)} MSE.}
\label{fig:performance_lmbda_1d}
\end{figure}

From \autoref{fig:performance_lmbda_1d} it is evident that Lasso with $\lambda=10^{-2}$ achieves an excellent predictive ability on the test set and by far surpasses the other models for all values of $\lambda$. Using to strong regularization causes the MSE to increase as the model becomes unable to fit the data. It is also noteworthy that OLS and Ridge has notable deviations between the training and test set, which may be an indication of overfitting.

The bias-variance decomposition of Ridge and Lasso regression with different regularization strengths are shown in \autoref{fig:bias_var__1d}.

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/ising1D_ridge_bias_variance.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/ising1D_lasso_bias_variance.pdf}}}
\caption{Bias-variance decomposition using bootstrap for \textbf{(a)} Ridge regression and \textbf{(b)} Lasso regression with different regularization strengths $\lambda$.}
\label{fig:bias_var__1d}
\end{figure}

The bias-variance decomposition corresponds well with the findings from \autoref{fig:performance_lmbda_1d}, which is expected as there is no noise in the data.

%----------------------------------------------------------------
\subsection{Identifying 2D Ising Model Phases with Logistic Regression}\label{sec:results logreg}
%----------------------------------------------------------------
Accompanying notebook: \href{https://github.com/nicolossus/FYS-STK4155-Project2/blob/master/notebooks/logreg_ising.ipynb}{logreg\_ising.ipynb}. 

\autoref{fig:gd_acc_lambda} shows the accuracy of the logistic regressor with Gradient Descent as optimization method and different values for the learning rate $\eta$ and regularization $\lambda$.

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.55]{latex/figures/log_acc_eta_0_0001.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.55]{latex/figures/log_acc_eta_0_001.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.55]{latex/figures/log_acc_eta_0_01.pdf}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.55]{latex/figures/log_acc_eta_0_1.pdf}}}
\caption{The accuracy score of the training, test and critical datasets as a function of the regularization parameter $\lambda$ for different learning rates $\eta$. Gradient descent was used as optimization method.}
\label{fig:gd_acc_lambda}
\end{figure}

In the above figure, we see the accuracies for the training (blue), test (red) and critical (green) datasets. The accuracies fluctuate dramatically, implying that the predictions are all over the place. However, the accuracies seems to be worse with stronger regularization. The difference between test and training sets could in theory tell us about the relative degree of overfitting, but is hard to assess due to the fluctuations. Due to the physics of the Ising model, we expect that predicting the phase close to the critical point will be more difficult. This, however, is not present in our results here, as the regressor seems to just guess for all sets.

There are no clear optimal parameters, and none of the best accuracies are great either. However, we stick with the best parameters for the maximum training accuracy, $\eta=0.01$ and $\lambda=0.1$, and try to fit on a larger training set (20\%). The result is shown in the ROC curve in \autoref{fig:roc_gd}.

\begin{figure}[H]
\begin{center}\includegraphics[scale=0.5]{latex/figures/logistic_roc_curve_gd.pdf}
\end{center}
\caption{ROC curve for our logistic regressor with Gradient Descent as optimization method.}
\label{fig:roc_gd}
\end{figure}

As seen in the above figure, our logistic regression model with gradient descent does no better than just guessing the phase.

A second optimization method has also been implemented, NR.
This method is notoriously slow iterative scheme, so we will only apply it on a small training set and with few iterations. Only 4\% training data and 10 iterations 

The results from running Newton-Raphson for different $\lambda$ is shown in \cref{tab:nr_res} and in \autoref{fig:acc_nr}. We get a slight top at $\lambda = 10^0=1$, so this is our chosen regularization parameter. We plot the ROC curve with this in \autoref{fig:roc_nr} to evaluate the model.


\begin{table}[H]
\caption{The logistic regressor's accuracy on the training, test and critical sets with Newton-Raphson as optimization method.}
\centering
\rowcolors{2}{gray!25}{white}
\input{latex/tables/nr_accuracy.tex}
\label{tab:nr_res}
\end{table}

\begin{figure}[H]
\begin{center}\includegraphics[scale=0.7]{latex/figures/log_acc_nr.pdf}
\end{center}
\caption{The accuracy score of the training, test and critical datasets as a function of the regularization parameter $\lambda$ for different learning rates $\eta$. Newton-Raphson was used as optimization method.}
\label{fig:acc_nr}
\end{figure}

\begin{figure}[H]
\begin{center}\includegraphics[scale=0.5]{latex/figures/logistic_roc_curve_nr.pdf}
\end{center}
\caption{ROC curve for our logistic regressor with Newton-Raphson as optimization method.}
\label{fig:roc_nr}
\end{figure}

As we can see in \cref{fig:roc_nr}, logistic regression is not able to correctly fit the Ising model. This is not surprising as it is not a linear model. The logistic regression model is no better than just guessing.

%----------------------------------------------------------------
\subsection{Regression on Energy of Generalized Ising Model using Neural Networks}\label{sec:results NN reg}
%----------------------------------------------------------------
Accompanying notebook: \href{https://github.com/nicolossus/FYS-STK4155-Project2/blob/master/notebooks/nn_ising.ipynb}{nn\_ising.ipynb}.

\autoref{fig:NN_reg} shows the R2-score different neural networks predicting the energy of the Ising model, using features produced by the Generalized Ising model.

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_reg_train}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_reg_test}}}
\caption{R2-score of several neural networks trained and tested on energies produced using the real Ising model \autoref{eq:ising 1D energy}, spin features produced using the generalized Ising model \autoref{eq:general}. All networks have a single hidden layer of 400 neurons, using $\tanh$ as activation, squared loss as cost function, and L2-regularization. They were trained using a grid search on learning rate and penalty}
\label{fig:NN_reg}
\end{figure}

As the real Ising model only include local coupling, only $80$ of the $1600$ features of the generalized model actually contribute to the energy. The interesting question to explore is if the network is able to pick out the useful features and ignore the others. From the above figure, we see that the more heavily penalised models perform generally better. In the same manner that Ridge and Lasso help linear regression suppress features of less importance and reduce variance, the regularizing the network produces better models, presumably by suppressing the weights connecting to the redundant features. To go a step further, a model using $\mu = 9 \cdot 10^{-5}$ and $\lambda = 0.004$ was trained. This model yielded a $R^2$ score of 0.26. 

To elaborate the previous point, figure \autoref{fig:NN_CS} visualises the Connection Strength (CS), \autoref{eq:CS}, of all the inputs for models using learning rate $\mu = 9 \cdot 10^{-5}$ and penalties $0.0005$, $0.001$, $0.002$ and $0.004$. 

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/pen1.png}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/pen2.png}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/pen3.png}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/pen4.png}}}
\caption{Connection strength \autoref{eq:CS} of all inputs for the models of learning rate $\mu = 9e-5$ and penalties $0.0005$, $0.001$, $0.002$ and $0.004$. The red dots highlights the CS of the features that contribute to the energy. The horizontal lines are the average CS of the two groups just described, respectively.}
\label{fig:NN_CS}
\end{figure}

As suspected, the increase in penalty raises the average CS of the inputs that contribute to the energy, making the network emphasize these inputs more. This ultimately yields a better model, until the penalty is raised to $0.004$. From the above figure, the contributing features are separated from the others more than ever. However, the weights have been shrunk to the point where the model is fairly desensitized to the features, with a CS of the order 0.3 instead of 3. Not too surprisingly, the model performed terribly, with a R2-score of $0.26$.

As a final note to the exploration of the learning behaviour, figure \autoref{fig:NN_learn} shows the $R^2$ score on validation data progressively as the network is trained.

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_learn.png}}}
\caption{R2-score of neural networks evaluated on a independent validation set. The models all use penalty $\lambda = 0.001$, with learning rate $\mu = 7e-5$, $9e-5$ and $1.1e-4$.}
\label{fig:NN_learn}
\end{figure}

The plot highlights the typical problems of tuning learning rate: The model with the smallest learning rate is constantly improving, but never reaching its potential since the step size is too small. On the other hand, the one with the highest learning rate might be continuously skipping over its local minima after getting fairly close at first, resulting in gradually degrading results.   


%----------------------------------------------------------------
\subsection{Identifying 2D Ising Model Phases with Neural Networks}\label{sec:results NN reg}
%----------------------------------------------------------------

\autoref{fig:NN_class} shows the accuracy score of several neural networks trained and tested on the combined ordered and disordered set from Mehta et al. in \cite{Mehta_2019}. Additionally, the models were tested on the critical set to explore the generalizability of the models.

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_class1.png}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_class2.png}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_class3.png}}}
\caption{Accuracy score of several neural networks trained and tested on the combined ordered and disordered set from Mehta. Additionally, the models were tested on the critical set. All networks have a single hidden layer of 400 neurons, using sigmoid as activation on both the hidden layer and output. They were trained using a grid search on learning rate and penalty(L2-regularization)}
\label{fig:NN_class}
\end{figure}

An eye-catching result is the sudden ruination of one of the models, yielding a accuracy score of precisely zero. This is curious, since models with parameters in the same neighborhood yield sensible predictions. When the faulty model was trained, the program once encountered "value encountered in true divide". This is a weakness of the cross-entropy. If the network misclassifies a label, and if the network is very sure in its (mis)classification, the derivative of the cross-entropy becomes enormous. This will likely ruin the weights during the next GD step, and the model too.

The fluke aside, our model attained a maximum accuracy score of $100\%$ for the ordered/disordered data meant for testing, rivaling Mehta's model of $99.9\%$ accuracy. However, our models did not generalize as good on the critical data, attaining only an accuracy of $76.5\%$ as opposed to $95.9\%$. This could be that 400 nodes is not high enough complexity to capture the finer details needed to distinguish the critical states, or it could simply be because the learning rate and penalty was not chosen precise enough: Since the training is very expensive, our grid search is rather course.

\autoref{fig:NN_misclass} highlights two states from the critical set that was misclassified by the best neural network trained on the ordered/disordered set.

\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_misclass.png}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{latex/figures/NN_misclass2.png}}}
\caption{Examaple of two states from the critical set misclassified by the best network trained on the ordered/disordered set.}
\label{fig:NN_misclass}
\end{figure}

Purely visually, one can see where the ambiguity in classification occurs: The misclassified states looks locally ordered in some places and disordered in other. This ambiguity can be theorized to confuse the neural network and produce misclassification.  

