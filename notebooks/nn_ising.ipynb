{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks for regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set fontsizes in figures\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize': 'large',\n",
    "          'xtick.labelsize': 'large',\n",
    "          'ytick.labelsize': 'large',\n",
    "          'legend.fontsize': 'large',\n",
    "          'legend.handlelength': 2}\n",
    "plt.rcParams.update(params)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code from src\n",
    "sys.path.insert(0, '../src/')\n",
    "from project_tools import fig_path\n",
    "\n",
    "from neuralnetwork import *\n",
    "from isingmodel import generate_1Ddata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data for 1D Ising-model energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1600)\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "np.random.seed(42)\n",
    "L = 40     # system size\n",
    "N = 10000  # number of points\n",
    "data, target = generate_1Ddata(L, N)\n",
    "target = target/np.max(target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, train_size=0.8, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_nn(learning_rate, penalty, layers, activ_func, cost, batch_size, epochs, X_train, y_train):\n",
    "    count = 0\n",
    "    tot_steps = len(learning_rate)*len(penalty)\n",
    "\n",
    "    models = []\n",
    "    #training models\n",
    "    for lr in learning_rate:\n",
    "        for p in penalty:\n",
    "            count += 1\n",
    "            \n",
    "            models.append(NeuralNetwork(layers, activ_func, cost))\n",
    "            models[-1].train(X_train, y_train, lr, p, batch_size, epochs)\n",
    "            \n",
    "            if count % (tot_steps / 100) == 0:\n",
    "                sys.stdout.write(\"\\r\" + \"%d \\%\" % (100 * count / tot_steps))\n",
    "                sys.stdout.flush()\n",
    "    \n",
    "    return models\n",
    "\n",
    "def R2(y, y_pred):\n",
    "    y_mean = np.mean(y)\n",
    "    return 1 - np.sum((y - y_pred)**2) /np.sum((y - y_mean)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify neural network for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99"
     ]
    }
   ],
   "source": [
    "tanh = Tanh()\n",
    "sig = Sigmoid()\n",
    "relu = Relu()\n",
    "_pass = Pass()\n",
    "square_loss = SquareLoss()\n",
    "np.random.seed(42)\n",
    "\n",
    "layers = [1600, 1000, 100, 1]\n",
    "activ_func = [tanh, tanh, _pass]\n",
    "cost = square_loss\n",
    "\n",
    "learning_rate = [0.00002]\n",
    "penalty =       [0.0001]\n",
    "\n",
    "models = grid_search_nn(learning_rate, penalty, layers, activ_func, cost, 100, 100, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05155751  0.2486871  -0.14854995  0.01988391 -0.15335323  0.15933833\n",
      " -0.10666503 -0.09685036  0.09591001  0.06116119]\n",
      "[-0.16666667  0.         -0.33333333  0.         -0.5         0.\n",
      " -0.5         0.          0.          0.        ]\n",
      "[0.38593271981205657]\n",
      "[0.3507209828418437]\n"
     ]
    }
   ],
   "source": [
    "R2_train = []\n",
    "R2_test = []\n",
    "for i in range(len(models)):\n",
    "    y_pred = models[i].predict(X_train)[:,0]\n",
    "    R2_train.append(R2(y_train, y_pred)) \n",
    "    print(y_pred[100:110])\n",
    "    print(y_train[100:110])\n",
    "    \n",
    "    y_pred = models[i].predict(X_test)[:,0]\n",
    "    R2_test.append(R2(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print(R2_train)\n",
    "print(R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([1500, 100, 1], [sig, _pass], square_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data and save locally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    url_main = \"https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/\"\n",
    "    data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\"\n",
    "    label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
    "\n",
    "    labels = pickle.load(urlopen(url_main + label_file_name))\n",
    "\n",
    "    data = pickle.load(urlopen(url_main + data_file_name))\n",
    "    data = np.unpackbits(data).reshape(-1, 1600)\n",
    "    data = data.astype('int')\n",
    "\n",
    "    np.save(\"labels\", labels)\n",
    "    np.save(\"spin_data\", data)\n",
    "\n",
    "    del data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"spin_data.npy\")\n",
    "y = np.load(\"labels.npy\")\n",
    "\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=X[:70000,:]\n",
    "y_ordered=y[:70000]\n",
    "\n",
    "X_critical=X[70000:100000,:]\n",
    "y_critical=y[70000:100000]\n",
    "\n",
    "X_disordered=X[100000:,:]\n",
    "y_disordered=y[100000:]\n",
    "\n",
    "# Remove critical, since we only want to train on ordered and disorderes\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "y=np.concatenate((y_ordered,y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.1,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the neural network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-15f1cd0d1d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-61022f58d5fd>\u001b[0m in \u001b[0;36mgrid_search_nn\u001b[0;34m(learning_rate, penalty, layers, activ_func, cost, batch_size, epochs, X_train, y_train)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtot_steps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FYS-STK4155-Project2/src/neuralnetwork.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, mu, lamb, batch_size, epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tanh = Tanh()\n",
    "sig = Sigmoid()\n",
    "relu = Relu()\n",
    "cross_entropy = CrossEntropy()\n",
    "\n",
    "layers = [1600, 400, 1]\n",
    "activ_func = [sig, sig, sig]\n",
    "cost = cross_entropy\n",
    "\n",
    "learning_rate = [0.00003]\n",
    "penalty =       [0.001]\n",
    "\n",
    "np.random.seed(42)\n",
    "models = grid_search_nn(learning_rate, penalty, layers, activ_func, cost, 100, 100, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97922923 0.97922923 0.97922923 0.06914239 0.97922923 0.04416221\n",
      " 0.97919212 0.01236772 0.0153758  0.01281863]\n",
      "[1 1 1 0 1 0 1 0 0 0]\n",
      "0.9582307692307692\n"
     ]
    }
   ],
   "source": [
    "y_pred = models[-1].predict(X_test)[:,0]\n",
    "print(y_pred[:10])\n",
    "print(y_test[:10])\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "success = np.sum(y_pred == y_test)\n",
    "print(success/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
